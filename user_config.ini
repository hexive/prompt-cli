###########################
#  prompt-cli user settings
###########################
# for each section just uncomment the setting you'd like to change
# from default. this config file should persist over updates.


###########################
#  VECTOR SEARCH (Qdrant)
###########################
[search]

# if you run qdrant at different
# ip address or use a different port
#qdrant_url = http://localhost:6333

# this is a public dataset of promps mostly from civitai
# so it's very NSFW / NSFL. If you want to tone that down
# a little you can use a blocklist which will filter searches
# to exclude keywords (this list was made by llama70b haha)
# edit prompt-cli/blacklist.txt one word or phrase per line.
#use_blocklist = false

# prompt-cli can start qdrant for you at launch
# the full run command used is:
# docker run -p 6333:6333 -p 6334:6334 -v qdrant_storage:/qdrant/storage:z 
# -e QDRANT__TELEMETRY_DISABLED=true qdrant/qdrant
# if you need to modify that its at app/preflight.py start_qdrant()
#qdrant_start = false


###########################
#     llama.cpp llm
###########################
[llm]

# These are Llama.cpp settings from the API. If you need
# to add others look in /app/llm.py get_model() 

# I like this model because it has a massive context size
# but that also eats VRAM so if you need to, you can 
# adjust the context to best suit your VRAM n_ctx (4096,8192,16384,32768)
#n_ctx=8192

# another way to lower VRAM usage is to decrease the number 
# of gpu layers used. -1 means use all layers. this model has 33 
#n_gpu_layers=-1 

# these you can ignore if you don't have multiple gpu. 
# split mode 1 is equally split model across gpus 0 is don't split
# main_gpu is just which one to use if it's not split 
#split_mode=1
#main_gpu=0

# true turns on all the llama.cpp feedback when loading the model
# and the token count for each response. ugly, but useful for debuging
#verbose=false


###########################
#     Stable Diffusion
###########################
[image]

# if you run stable-diffusion-webui at different
# ip address or use a different port
#image-gen_url = http://localhost:7860

# true saves all images generated to prompt-cli/output 
#save_images_app = false

# true saves all images generated to stable-diffusion-webui/output
#save_images_api = false

# prompt-cli can start stable-diffusion for you at launch
# update with full path to the stable-diffusions webui.sh
# including any launch flags you'd like. --api is required

#stable_diffusion_start = false
#stable_diffusion_path = /home/your/full/path/to/stable-diffusion-webui/webui.sh --api


###########################
#     User Interface
###########################
[ui]

# 16 colors is all you'1l ever need! valid names:
# black, red, green, yellow, blue, magenta, cyan, white
# bright_black, bright_red, bright_green, bright_yellow, 
# bright_blue, bright_magenta, bright_cyan, bright_white

#app_color = blue
#search_color = cyan
#llm_color = green
#error_color = red
